#!/usr/bin/env python3
"""
╔══════════════════════════════════════════════════════════════════════════════════════════╗
║  ✧∿✧ XENOMORPHIC NEURAL NETWORK v3.0 - HOLORAID EDITION ✧∿✧                             ║
║══════════════════════════════════════════════════════════════════════════════════════════║
║                                                                                          ║
║  SAFEGEAR PRIMITIVES:                                                                    ║
║  • Bijective winding transformations via coprime modular systems                         ║
║  • Gear ratios derived from coprime pairs for information-preserving transforms          ║
║  • Reversible encoding using modular arithmetic                                          ║
║                                                                                          ║
║  HOLORAID ENCODING SYSTEM:                                                               ║
║  • Chinese Remainder Theorem-based erasure coding                                        ║
║  • Holographic principle: each shard contains full information                           ║
║  • k-of-n threshold reconstruction with exact recovery                                   ║
║  • Maximum Distance Separable (MDS) properties                                           ║
║                                                                                          ║
║  NEURAL NETWORK INTEGRATION:                                                             ║
║  • SafeGear Layer - bijective feature transformations                                    ║
║  • HoloRAID Layer - distributed holographic representations                              ║
║  • CRT Reconstruction Layer - information fusion from any k shards                       ║
║  • Full backpropagation through holographic encoding                                     ║
║                                                                                          ║
║  Based on Shaun's HyperMorphic Mathematics framework                                     ║
╚══════════════════════════════════════════════════════════════════════════════════════════╝
"""

import numpy as np
import time
from typing import Tuple, List, Dict, Optional, Any
from abc import ABC, abstractmethod
from functools import reduce
import warnings
warnings.filterwarnings('ignore')

np.random.seed(42)

# ═══════════════════════════════════════════════════════════════════════════════
# MATHEMATICAL CONSTANTS
# ═══════════════════════════════════════════════════════════════════════════════

PHI = (1.0 + np.sqrt(5)) / 2.0          # Golden ratio
PSI = 1.0 / PHI                          # Golden ratio conjugate
PLASTIC = 1.324717957244746              # Plastic constant (x³ = x + 1)
EULER_MASCHERONI = 0.5772156649015329    # Euler-Mascheroni constant
FEIGENBAUM_DELTA = 4.669201609102990     # Feigenbaum constant

# First 50 primes for HoloRAID encoding
PRIMES = np.array([
    2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47,
    53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113,
    127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197,
    199, 211, 223, 227, 229
], dtype=np.int64)


# ═══════════════════════════════════════════════════════════════════════════════
# SAFEGEAR PRIMITIVES - BIJECTIVE WINDING TRANSFORMATIONS
# ═══════════════════════════════════════════════════════════════════════════════

class SafeGear:
    """
    SafeGear: Bijective winding transformations using coprime modular systems.
    
    A SafeGear is defined by a pair of coprime integers (teeth_a, teeth_b) that
    create a bijective mapping through their gear ratio. The key insight is that
    coprime pairs guarantee reversibility via the Extended Euclidean Algorithm.
    
    Properties:
    - Bijective: Every input maps to exactly one output, and vice versa
    - Reversible: Original value can be exactly reconstructed
    - Interference-free: Coprimality ensures no information loss
    """
    
    def __init__(self, teeth_a: int, teeth_b: int):
        """
        Initialize SafeGear with coprime teeth counts.
        
        Args:
            teeth_a: First gear teeth count (must be coprime with teeth_b)
            teeth_b: Second gear teeth count (must be coprime with teeth_a)
        """
        if np.gcd(teeth_a, teeth_b) != 1:
            raise ValueError(f"Teeth counts must be coprime: gcd({teeth_a}, {teeth_b}) != 1")
        
        self.teeth_a = teeth_a
        self.teeth_b = teeth_b
        self.modulus = teeth_a * teeth_b  # Combined cycle length
        self.gear_ratio = teeth_a / teeth_b
        
        # Precompute modular inverses using Extended Euclidean Algorithm
        self.inv_a = self._mod_inverse(teeth_a, teeth_b)
        self.inv_b = self._mod_inverse(teeth_b, teeth_a)
    
    @staticmethod
    def _extended_gcd(a: int, b: int) -> Tuple[int, int, int]:
        """Extended Euclidean Algorithm: returns (gcd, x, y) where ax + by = gcd"""
        if a == 0:
            return b, 0, 1
        gcd, x1, y1 = SafeGear._extended_gcd(b % a, a)
        x = y1 - (b // a) * x1
        y = x1
        return gcd, x, y
    
    @staticmethod
    def _mod_inverse(a: int, m: int) -> int:
        """Compute modular multiplicative inverse of a mod m"""
        gcd, x, _ = SafeGear._extended_gcd(a % m, m)
        if gcd != 1:
            raise ValueError(f"Modular inverse doesn't exist for {a} mod {m}")
        return (x % m + m) % m
    
    def wind(self, position: np.ndarray) -> np.ndarray:
        """
        Wind transformation: Apply bijective gear transformation.
        
        Uses interleaved transformation based on gear ratio.
        For position p in [0, a*b), compute:
        q, r = divmod(p, b)  # quotient and remainder
        wound = r * a + q    # interleave based on gear ratio
        """
        pos = position.astype(np.int64) % self.modulus
        q = pos // self.teeth_b
        r = pos % self.teeth_b
        wound = (r * self.teeth_a + q) % self.modulus
        return wound
    
    def unwind(self, wound: np.ndarray) -> np.ndarray:
        """
        Unwind transformation: Exact inverse of wind operation.
        
        Reverses the interleaving:
        q', r' = divmod(wound, a)
        position = r' * b + q'
        """
        w = wound.astype(np.int64) % self.modulus
        q_prime = w // self.teeth_a
        r_prime = w % self.teeth_a
        unwound = (r_prime * self.teeth_b + q_prime) % self.modulus
        return unwound
    
    def continuous_wind(self, x: np.ndarray) -> np.ndarray:
        """
        Continuous winding for real-valued inputs.
        
        Applies sinusoidal modulation based on gear ratio for smooth,
        differentiable transformations.
        """
        phase = 2 * np.pi * x * self.gear_ratio
        return x + np.sin(phase) / self.teeth_b + np.cos(phase * PHI) / self.teeth_a
    
    def continuous_unwind(self, y: np.ndarray, iterations: int = 5) -> np.ndarray:
        """
        Approximate inverse of continuous winding via fixed-point iteration.
        """
        x = y.copy()
        for _ in range(iterations):
            phase = 2 * np.pi * x * self.gear_ratio
            correction = np.sin(phase) / self.teeth_b + np.cos(phase * PHI) / self.teeth_a
            x = y - correction
        return x


class SafeGearChain:
    """
    Chain of SafeGears for complex bijective transformations.
    
    Multiple SafeGears can be chained to create more complex mappings
    while preserving bijectivity. The chain's total modulus is the
    product of all individual moduli.
    """
    
    def __init__(self, gear_pairs: List[Tuple[int, int]]):
        """
        Initialize chain with list of coprime pairs.
        
        Args:
            gear_pairs: List of (teeth_a, teeth_b) coprime pairs
        """
        self.gears = [SafeGear(a, b) for a, b in gear_pairs]
        self.total_modulus = reduce(lambda x, y: x * y, 
                                    [g.modulus for g in self.gears], 1)
    
    def wind(self, x: np.ndarray) -> np.ndarray:
        """Apply all gears in sequence"""
        result = x.copy()
        for gear in self.gears:
            result = gear.continuous_wind(result)
        return result
    
    def unwind(self, y: np.ndarray) -> np.ndarray:
        """Reverse all gears in reverse order"""
        result = y.copy()
        for gear in reversed(self.gears):
            result = gear.continuous_unwind(result)
        return result


# ═══════════════════════════════════════════════════════════════════════════════
# HOLORAID ENCODING - CRT-BASED ERASURE CODING
# ═══════════════════════════════════════════════════════════════════════════════

class HoloRAID:
    """
    HoloRAID: Holographic erasure coding using the Chinese Remainder Theorem.
    
    The key insight is that CRT provides a natural holographic encoding:
    - A value M < product(primes) can be represented as residues (r1, r2, ..., rn)
    - Any k residues can reconstruct M if product of their primes > M
    - Each shard contains "holographic" information about the whole
    
    Properties:
    - k-of-n threshold: Any k shards can reconstruct the original
    - Maximum Distance Separable (MDS): Optimal redundancy
    - Information-theoretic security: k-1 shards reveal nothing
    - Exact reconstruction: No approximation, mathematically perfect
    """
    
    def __init__(self, n_shards: int = 7, k_threshold: int = 3, 
                 prime_offset: int = 15, quantization_bits: int = 16):
        """
        Initialize HoloRAID encoder.
        
        Args:
            n_shards: Total number of shards to generate
            k_threshold: Minimum shards needed for reconstruction
            prime_offset: Starting index in PRIMES array (use larger primes)
            quantization_bits: Bits for quantizing continuous values
        """
        self.n_shards = n_shards
        self.k_threshold = k_threshold
        self.quantization_bits = quantization_bits
        self.max_quant = 2 ** quantization_bits - 1
        
        # Select n larger primes (start from offset to get primes > 50)
        # This ensures product of k primes > max_quant
        self.primes = PRIMES[prime_offset:prime_offset + n_shards].copy()
        
        # Verify CRT constraint: product of smallest k primes must exceed max value
        sorted_primes = np.sort(self.primes)[:k_threshold]
        self.modulus_k = int(np.prod(sorted_primes))
        self.modulus_n = int(np.prod(self.primes))
        
        if self.modulus_k <= self.max_quant:
            raise ValueError(f"Product of {k_threshold} smallest primes ({self.modulus_k}) must exceed max_quant ({self.max_quant}). Use larger primes.")
        
        # Precompute CRT coefficients for all possible k-subsets
        self._precompute_crt_coefficients()
    
    def _precompute_crt_coefficients(self):
        """Precompute CRT reconstruction coefficients for efficiency"""
        self.crt_cache = {}
        
        # For each shard, compute its contribution coefficient
        M = self.modulus_n
        self.crt_coefficients = []
        
        for i, p in enumerate(self.primes):
            Mi = M // p
            yi = pow(int(Mi), -1, int(p))  # Modular inverse of Mi mod p
            self.crt_coefficients.append(int(Mi * yi))
    
    def encode(self, data: np.ndarray) -> List[np.ndarray]:
        """
        Encode data into n holographic shards.
        
        Each shard contains residues that, when combined with k-1 others,
        can exactly reconstruct the original data.
        
        Args:
            data: Input array to encode (will be quantized if float)
            
        Returns:
            List of n shard arrays
        """
        # Quantize if continuous
        if data.dtype in [np.float32, np.float64]:
            # Normalize to [0, 1] then scale to quantization range
            data_min, data_max = data.min(), data.max()
            data_range = data_max - data_min + 1e-10
            normalized = (data - data_min) / data_range
            quantized = (normalized * self.max_quant).astype(np.int64)
            self._encode_metadata = {
                'min': data_min,
                'max': data_max,
                'shape': data.shape,
                'dtype': data.dtype
            }
        else:
            quantized = data.astype(np.int64)
            self._encode_metadata = {
                'shape': data.shape,
                'dtype': data.dtype
            }
        
        # Flatten for encoding
        flat = quantized.flatten()
        
        # Generate shards: each shard contains residues mod prime[i]
        shards = []
        for i, p in enumerate(self.primes):
            shard = flat % p
            shards.append(shard.astype(np.int64))
        
        return shards
    
    def decode(self, shards: List[np.ndarray], 
               shard_indices: Optional[List[int]] = None) -> np.ndarray:
        """
        Reconstruct data from k or more shards using CRT.
        
        Args:
            shards: List of shard arrays (at least k_threshold needed)
            shard_indices: Which shards are being used (indices into primes)
            
        Returns:
            Reconstructed data array
        """
        if shard_indices is None:
            shard_indices = list(range(len(shards)))
        
        if len(shard_indices) < self.k_threshold:
            raise ValueError(f"Need at least {self.k_threshold} shards, got {len(shard_indices)}")
        
        # Use first k shards for reconstruction
        k = self.k_threshold
        used_indices = shard_indices[:k]
        used_shards = shards[:k]
        used_primes = [int(self.primes[i]) for i in used_indices]
        
        # Compute combined modulus for these k primes
        M = 1
        for p in used_primes:
            M *= p
        
        # CRT reconstruction
        result = np.zeros(shards[0].shape, dtype=np.int64)
        
        for i, (shard, p) in enumerate(zip(used_shards, used_primes)):
            Mi = M // p
            # Modular inverse of Mi mod p
            yi = pow(int(Mi) % p, -1, p)
            contribution = (shard.astype(np.int64) * Mi * yi) % M
            result = (result + contribution) % M
        
        # Dequantize if needed
        if hasattr(self, '_encode_metadata') and 'min' in self._encode_metadata:
            meta = self._encode_metadata
            # Reshape and convert back to float
            result = result.reshape(meta['shape'])
            normalized = result.astype(np.float64) / self.max_quant
            reconstructed = normalized * (meta['max'] - meta['min']) + meta['min']
            return reconstructed
        
        return result.reshape(self._encode_metadata['shape'])
    
    def verify_shard(self, data: np.ndarray, shard: np.ndarray, 
                     shard_index: int) -> bool:
        """Verify a shard is consistent with original data"""
        expected = data.flatten() % self.primes[shard_index]
        return np.allclose(shard.flatten(), expected)
    
    def get_redundancy_factor(self) -> float:
        """Calculate storage overhead compared to original data"""
        return self.n_shards / self.k_threshold
    
    def get_failure_tolerance(self) -> int:
        """Number of shards that can fail while still allowing recovery"""
        return self.n_shards - self.k_threshold


class HolographicEncoder:
    """
    Neural-network-friendly holographic encoder using continuous approximations.
    
    This encoder provides differentiable operations for gradient-based learning
    while maintaining the holographic properties of HoloRAID.
    """
    
    def __init__(self, dim: int, n_shards: int = 5, n_harmonics: int = 8):
        """
        Initialize differentiable holographic encoder.
        
        Args:
            dim: Feature dimension
            n_shards: Number of holographic projections
            n_harmonics: Harmonics per shard for interference patterns
        """
        self.dim = dim
        self.n_shards = n_shards
        self.n_harmonics = n_harmonics
        
        # Reference beam phases (like holographic reference)
        self.ref_phases = np.random.uniform(0, 2*np.pi, (n_shards, n_harmonics))
        
        # Projection matrices for each shard (random orthogonal)
        self.projections = []
        for _ in range(n_shards):
            # Random orthogonal matrix via QR decomposition
            H = np.random.randn(dim, dim)
            Q, _ = np.linalg.qr(H)
            self.projections.append(Q)
        
        # Reconstruction weights (learned during training)
        self.recon_weights = np.ones((n_shards, dim)) / n_shards
    
    def encode(self, x: np.ndarray) -> List[np.ndarray]:
        """
        Encode input into holographic shards.
        
        Each shard contains an interference pattern that, when combined
        with reference beams, can reconstruct the original signal.
        """
        shards = []
        for i in range(self.n_shards):
            # Project through shard-specific rotation
            projected = x @ self.projections[i]
            
            # Create interference pattern with reference beam
            interference = np.zeros_like(projected)
            for h in range(self.n_harmonics):
                freq = (h + 1) * PHI
                phase = self.ref_phases[i, h]
                interference += np.sin(projected * freq + phase) / (h + 1)
            
            # Combine projection and interference
            shard = projected + interference * 0.1
            shards.append(shard)
        
        return shards
    
    def decode(self, shards: List[np.ndarray], 
               available_indices: Optional[List[int]] = None) -> np.ndarray:
        """
        Reconstruct from available shards.
        
        Uses weighted combination of inverse projections with
        interference cancellation.
        """
        if available_indices is None:
            available_indices = list(range(len(shards)))
        
        reconstruction = np.zeros_like(shards[0])
        total_weight = 0
        
        for idx in available_indices:
            shard = shards[idx]
            
            # Inverse projection (orthogonal matrix -> transpose is inverse)
            inv_proj = shard @ self.projections[idx].T
            
            # Weight by reconstruction coefficients
            weight = np.mean(self.recon_weights[idx])
            reconstruction += inv_proj * weight
            total_weight += weight
        
        return reconstruction / (total_weight + 1e-10)


# ═══════════════════════════════════════════════════════════════════════════════
# NEURAL NETWORK UTILITIES
# ═══════════════════════════════════════════════════════════════════════════════

def epsilon_safe(x, eps=1e-8):
    signs = np.sign(x)
    signs = np.where(signs == 0, 1, signs)
    return np.where(np.abs(x) < eps, eps * signs, x)

def stable_softmax(x, axis=-1):
    x_max = np.max(x, axis=axis, keepdims=True)
    exp_x = np.exp(x - x_max)
    return exp_x / (np.sum(exp_x, axis=axis, keepdims=True) + 1e-10)

def stable_sigmoid(x):
    pos_mask = x >= 0
    neg_mask = ~pos_mask
    result = np.zeros_like(x)
    result[pos_mask] = 1 / (1 + np.exp(-np.clip(x[pos_mask], -500, 500)))
    exp_x = np.exp(np.clip(x[neg_mask], -500, 500))
    result[neg_mask] = exp_x / (1 + exp_x)
    return result

def gelu(x):
    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))

def gelu_derivative(x):
    cdf = 0.5 * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))
    pdf = np.exp(-0.5 * x**2) / np.sqrt(2 * np.pi)
    return cdf + x * pdf


# ═══════════════════════════════════════════════════════════════════════════════
# NEURAL NETWORK LAYER BASE
# ═══════════════════════════════════════════════════════════════════════════════

class Layer(ABC):
    def __init__(self, name="layer"):
        self.name = name
        self.params = {}
        self.grads = {}
        self.cache = {}
        self.training = True
        self._param_lr_scale = {}
    
    @abstractmethod
    def forward(self, x): pass
    
    @abstractmethod
    def backward(self, grad): pass
    
    def set_training(self, mode):
        self.training = mode
    
    def get_param_lr_scale(self, param_name):
        return self._param_lr_scale.get(param_name, 1.0)


# ═══════════════════════════════════════════════════════════════════════════════
# STANDARD NEURAL NETWORK LAYERS
# ═══════════════════════════════════════════════════════════════════════════════

class Dense(Layer):
    def __init__(self, in_dim, out_dim, use_bias=True):
        super().__init__("Dense")
        limit = np.sqrt(6 / (in_dim + out_dim))
        self.params['W'] = np.random.uniform(-limit, limit, (in_dim, out_dim))
        if use_bias:
            self.params['b'] = np.zeros(out_dim)
        self.use_bias = use_bias
    
    def forward(self, x):
        self.cache['input'] = x
        out = x @ self.params['W']
        if self.use_bias:
            out = out + self.params['b']
        return out
    
    def backward(self, grad):
        x = self.cache['input']
        if x.ndim == 1:
            x = x.reshape(1, -1)
            grad = grad.reshape(1, -1)
        self.grads['W'] = x.T @ grad
        if self.use_bias:
            self.grads['b'] = np.sum(grad, axis=0)
        return grad @ self.params['W'].T


class LayerNorm(Layer):
    def __init__(self, dim, eps=1e-5):
        super().__init__("LayerNorm")
        self.eps = eps
        self.dim = dim
        self.params['gamma'] = np.ones(dim)
        self.params['beta'] = np.zeros(dim)
    
    def forward(self, x):
        self.cache['input'] = x
        mean = np.mean(x, axis=-1, keepdims=True)
        var = np.var(x, axis=-1, keepdims=True)
        self.cache['mean'] = mean
        self.cache['var'] = var
        x_norm = (x - mean) / np.sqrt(var + self.eps)
        self.cache['x_norm'] = x_norm
        return self.params['gamma'] * x_norm + self.params['beta']
    
    def backward(self, grad):
        x = self.cache['input']
        x_norm = self.cache['x_norm']
        var = self.cache['var']
        self.grads['gamma'] = np.sum(grad * x_norm, axis=0) if grad.ndim > 1 else grad * x_norm
        self.grads['beta'] = np.sum(grad, axis=0) if grad.ndim > 1 else grad
        N = x.shape[-1]
        std_inv = 1 / np.sqrt(var + self.eps)
        dx_norm = grad * self.params['gamma']
        dvar = np.sum(dx_norm * (x - self.cache['mean']) * -0.5 * std_inv**3, axis=-1, keepdims=True)
        dmean = np.sum(dx_norm * -std_inv, axis=-1, keepdims=True)
        dx = dx_norm * std_inv + dvar * 2 * (x - self.cache['mean']) / N + dmean / N
        return dx


class Dropout(Layer):
    def __init__(self, p=0.1):
        super().__init__("Dropout")
        self.p = p
    
    def forward(self, x):
        if self.training and self.p > 0:
            mask = (np.random.random(x.shape) > self.p).astype(float)
            self.cache['mask'] = mask
            return x * mask / (1 - self.p)
        return x
    
    def backward(self, grad):
        if self.training and self.p > 0:
            return grad * self.cache['mask'] / (1 - self.p)
        return grad


class Activation(Layer):
    def __init__(self, activation='gelu'):
        super().__init__(f"Activation_{activation}")
        self.activation = activation
    
    def forward(self, x):
        self.cache['input'] = x
        if self.activation == 'gelu':
            return gelu(x)
        elif self.activation == 'relu':
            return np.maximum(0, x)
        elif self.activation == 'tanh':
            return np.tanh(x)
        return x
    
    def backward(self, grad):
        x = self.cache['input']
        if self.activation == 'gelu':
            return grad * gelu_derivative(x)
        elif self.activation == 'relu':
            return grad * (x > 0).astype(float)
        elif self.activation == 'tanh':
            return grad * (1 - np.tanh(x)**2)
        return grad


# ═══════════════════════════════════════════════════════════════════════════════
# SAFEGEAR NEURAL NETWORK LAYER
# ═══════════════════════════════════════════════════════════════════════════════

class SafeGearLayer(Layer):
    """
    Neural network layer using SafeGear bijective transformations.
    
    This layer applies learnable gear-based winding transformations to features,
    creating reversible non-linear mappings while preserving information.
    """
    
    def __init__(self, dim: int, n_gears: int = 4):
        """
        Initialize SafeGear layer.
        
        Args:
            dim: Feature dimension
            n_gears: Number of gear pairs to chain
        """
        super().__init__("SafeGear")
        self.dim = dim
        self.n_gears = n_gears
        
        # Initialize gear parameters (coprime pairs)
        # We use consecutive Fibonacci-like numbers which are often coprime
        base_gears = [(3, 5), (5, 7), (7, 11), (11, 13), (13, 17), (17, 19)]
        self.gear_pairs = base_gears[:n_gears]
        
        # Create gear chain
        self.gear_chain = SafeGearChain(self.gear_pairs)
        
        # Learnable mixing parameters
        self.params['gear_weights'] = np.ones(n_gears) / n_gears
        self.params['gear_phases'] = np.zeros(n_gears)
        self.params['gear_scale'] = np.array([1.0])
        
        # Feature-wise learnable amplitudes
        self.params['amplitudes'] = np.ones(dim) * 0.1
        
        self._param_lr_scale = {
            'gear_weights': 0.5,
            'gear_phases': 0.3,
            'gear_scale': 0.1,
            'amplitudes': 0.5
        }
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache['input'] = x
        
        # Apply gear chain winding
        wound = self.gear_chain.wind(x * self.params['gear_scale'][0])
        
        # Combine with phase-shifted versions
        result = x.copy()
        for i, gear in enumerate(self.gear_chain.gears):
            phase = self.params['gear_phases'][i]
            weight = self.params['gear_weights'][i]
            
            # Phase-shifted winding
            shifted = gear.continuous_wind(x + phase)
            result = result + weight * (shifted - x) * self.params['amplitudes']
        
        self.cache['wound'] = wound
        self.cache['result'] = result
        
        return result
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        x = self.cache['input']
        
        # Simplified gradient (full derivative is complex)
        dx = grad.copy()
        
        # Gradient w.r.t. amplitudes
        self.grads['amplitudes'] = np.zeros(self.dim)
        for i, gear in enumerate(self.gear_chain.gears):
            phase = self.params['gear_phases'][i]
            weight = self.params['gear_weights'][i]
            shifted = gear.continuous_wind(x + phase)
            self.grads['amplitudes'] += np.sum(grad * weight * (shifted - x), axis=0) if grad.ndim > 1 else grad * weight * (shifted - x)
        
        self.grads['gear_weights'] = np.zeros(self.n_gears)
        self.grads['gear_phases'] = np.zeros(self.n_gears)
        self.grads['gear_scale'] = np.array([0.0])
        
        return dx


# ═══════════════════════════════════════════════════════════════════════════════
# HOLORAID NEURAL NETWORK LAYER
# ═══════════════════════════════════════════════════════════════════════════════

class HoloRAIDLayer(Layer):
    """
    Neural network layer implementing HoloRAID holographic encoding.
    
    This layer distributes information across multiple "shards" using
    CRT-inspired transformations, creating redundant representations
    that can be reconstructed from subsets.
    """
    
    def __init__(self, dim: int, n_shards: int = 5, k_threshold: int = 3):
        """
        Initialize HoloRAID layer.
        
        Args:
            dim: Feature dimension
            n_shards: Number of holographic shards
            k_threshold: Minimum shards for reconstruction
        """
        super().__init__("HoloRAID")
        self.dim = dim
        self.n_shards = n_shards
        self.k_threshold = k_threshold
        
        # Use primes as moduli for CRT-like operations
        self.primes = PRIMES[:n_shards].astype(np.float64)
        
        # Learnable shard weights for combining residues
        self.params['shard_weights'] = np.ones((n_shards, dim)) / n_shards
        
        # Phase offsets for each shard (holographic reference beam)
        self.params['shard_phases'] = np.random.uniform(0, 2*np.pi, (n_shards,))
        
        # Reconstruction mixing matrix
        self.params['recon_matrix'] = np.eye(dim) * 0.5 + np.random.randn(dim, dim) * 0.1
        
        # CRT-inspired coefficients (learnable)
        self.params['crt_coeffs'] = self._init_crt_coeffs()
        
        self._param_lr_scale = {
            'shard_weights': 0.5,
            'shard_phases': 0.3,
            'recon_matrix': 0.5,
            'crt_coeffs': 0.3
        }
    
    def _init_crt_coeffs(self) -> np.ndarray:
        """Initialize CRT-like coefficients based on prime moduli"""
        coeffs = np.zeros((self.n_shards, self.dim))
        M = np.prod(self.primes[:self.k_threshold])
        
        for i, p in enumerate(self.primes):
            Mi = M / p
            # Approximate modular inverse as phase
            coeffs[i] = Mi * np.sin(np.arange(self.dim) * np.pi / p + PHI)
        
        return coeffs / np.abs(coeffs).max()
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache['input'] = x
        
        # Generate shards via modular residue-like operations
        shards = []
        for i in range(self.n_shards):
            p = self.primes[i]
            phase = self.params['shard_phases'][i]
            
            # Continuous "residue" using sinusoidal modulation
            residue = np.sin(x * 2 * np.pi / p + phase)
            
            # Weight by shard-specific weights
            shard = residue * self.params['shard_weights'][i]
            shards.append(shard)
        
        self.cache['shards'] = shards
        
        # Holographic reconstruction: CRT-like combination
        reconstruction = np.zeros_like(x)
        for i, shard in enumerate(shards):
            reconstruction += shard * self.params['crt_coeffs'][i]
        
        # Final mixing through reconstruction matrix
        if x.ndim == 1:
            output = reconstruction @ self.params['recon_matrix']
        else:
            output = reconstruction @ self.params['recon_matrix']
        
        # Blend with original (residual-like connection)
        output = x + output * 0.5
        
        return output
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        x = self.cache['input']
        shards = self.cache['shards']
        
        # Gradient through reconstruction matrix
        if grad.ndim == 1:
            grad_recon = grad @ self.params['recon_matrix'].T * 0.5
        else:
            grad_recon = grad @ self.params['recon_matrix'].T * 0.5
        
        # Gradient w.r.t. reconstruction matrix
        reconstruction = np.zeros_like(x)
        for i, shard in enumerate(shards):
            reconstruction += shard * self.params['crt_coeffs'][i]
        
        if grad.ndim == 1:
            self.grads['recon_matrix'] = np.outer(reconstruction, grad * 0.5)
        else:
            self.grads['recon_matrix'] = reconstruction.T @ (grad * 0.5)
        
        # Gradient w.r.t. CRT coefficients
        self.grads['crt_coeffs'] = np.zeros_like(self.params['crt_coeffs'])
        for i, shard in enumerate(shards):
            if grad_recon.ndim == 1:
                self.grads['crt_coeffs'][i] = grad_recon * shard
            else:
                self.grads['crt_coeffs'][i] = np.sum(grad_recon * shard, axis=0)
        
        # Gradient w.r.t. shard weights and phases
        self.grads['shard_weights'] = np.zeros_like(self.params['shard_weights'])
        self.grads['shard_phases'] = np.zeros(self.n_shards)
        
        dx = grad.copy()  # From residual connection
        
        for i in range(self.n_shards):
            p = self.primes[i]
            phase = self.params['shard_phases'][i]
            
            residue = np.sin(x * 2 * np.pi / p + phase)
            d_residue = np.cos(x * 2 * np.pi / p + phase) * 2 * np.pi / p
            
            grad_shard = grad_recon * self.params['crt_coeffs'][i]
            
            if grad_shard.ndim == 1:
                self.grads['shard_weights'][i] = grad_shard * residue
            else:
                self.grads['shard_weights'][i] = np.sum(grad_shard * residue, axis=0)
            
            dx += grad_shard * self.params['shard_weights'][i] * d_residue
        
        return dx


# ═══════════════════════════════════════════════════════════════════════════════
# CRT RECONSTRUCTION LAYER
# ═══════════════════════════════════════════════════════════════════════════════

class CRTReconstructionLayer(Layer):
    """
    Layer that implements exact CRT reconstruction from modular residues.
    
    This layer takes features and produces k separate "views" that can
    each independently reconstruct the original through CRT properties.
    """
    
    def __init__(self, dim: int, n_views: int = 3):
        """
        Initialize CRT reconstruction layer.
        
        Args:
            dim: Feature dimension
            n_views: Number of parallel reconstructions
        """
        super().__init__("CRTReconstruction")
        self.dim = dim
        self.n_views = n_views
        
        # Prime bases for each view
        self.primes = PRIMES[:n_views * 3].reshape(n_views, 3)
        
        # Learnable combination weights
        self.params['view_weights'] = np.ones(n_views) / n_views
        
        # Per-view transformation matrices
        for i in range(n_views):
            limit = np.sqrt(6 / (dim * 2))
            self.params[f'view_W{i}'] = np.random.uniform(-limit, limit, (dim, dim))
        
        self._param_lr_scale = {f'view_W{i}': 0.5 for i in range(n_views)}
        self._param_lr_scale['view_weights'] = 0.3
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache['input'] = x
        
        views = []
        for i in range(self.n_views):
            # Transform through view-specific matrix
            if x.ndim == 1:
                view = x @ self.params[f'view_W{i}']
            else:
                view = x @ self.params[f'view_W{i}']
            
            # Apply CRT-inspired modulation
            for j, p in enumerate(self.primes[i]):
                view = view + np.sin(view * 2 * np.pi / p) * (1 / (j + 1))
            
            views.append(view)
        
        self.cache['views'] = views
        
        # Weighted combination of views
        output = np.zeros_like(x)
        for i, view in enumerate(views):
            output += self.params['view_weights'][i] * view
        
        return output
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        x = self.cache['input']
        views = self.cache['views']
        
        # Gradient w.r.t. view weights
        self.grads['view_weights'] = np.zeros(self.n_views)
        for i, view in enumerate(views):
            self.grads['view_weights'][i] = np.sum(grad * view)
        
        # Gradient through each view
        dx = np.zeros_like(x)
        for i in range(self.n_views):
            grad_view = grad * self.params['view_weights'][i]
            
            # Simplified gradient through CRT modulation
            if x.ndim == 1:
                dx += grad_view @ self.params[f'view_W{i}'].T
                self.grads[f'view_W{i}'] = np.outer(x, grad_view)
            else:
                dx += grad_view @ self.params[f'view_W{i}'].T
                self.grads[f'view_W{i}'] = x.T @ grad_view
        
        return dx


# ═══════════════════════════════════════════════════════════════════════════════
# HYPERMORPHIC ACTIVATION LAYER
# ═══════════════════════════════════════════════════════════════════════════════

class HyperMorphicLayer(Layer):
    """
    HyperMorphic activation using golden ratio fractal scaling.
    """
    
    def __init__(self, dim: int, fractal_depth: int = 4):
        super().__init__("HyperMorphic")
        self.dim = dim
        self.fractal_depth = fractal_depth
        self.params['scale'] = np.array([np.log(dim + 1) * PHI])
        self.params['weights'] = np.array([0.1 * (PSI ** d) for d in range(fractal_depth)])
        self.params['phases'] = np.zeros(fractal_depth)
        self._param_lr_scale = {'scale': 0.1, 'weights': 0.5, 'phases': 0.3}
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache['input'] = x
        scale = self.params['scale'][0]
        result = x.copy()
        
        for d in range(self.fractal_depth):
            fractal_scale = scale * (PHI ** d)
            result = result + np.sin((x + self.params['phases'][d]) / fractal_scale) * self.params['weights'][d]
        
        return epsilon_safe(result)
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        x = self.cache['input']
        scale = self.params['scale'][0]
        
        dx = grad.copy()
        self.grads['scale'] = np.array([0.0])
        self.grads['weights'] = np.zeros(self.fractal_depth)
        self.grads['phases'] = np.zeros(self.fractal_depth)
        
        for d in range(self.fractal_depth):
            fractal_scale = scale * (PHI ** d)
            sin_term = np.sin((x + self.params['phases'][d]) / fractal_scale)
            cos_term = np.cos((x + self.params['phases'][d]) / fractal_scale)
            
            self.grads['weights'][d] = np.sum(grad * sin_term)
            dx += grad * cos_term * self.params['weights'][d] / fractal_scale
        
        return dx


# ═══════════════════════════════════════════════════════════════════════════════
# QUANTUM INTERFERENCE LAYER (from v2.0)
# ═══════════════════════════════════════════════════════════════════════════════

class QuantumInterferenceLayer(Layer):
    """
    Multi-reality quantum interference layer.
    """
    
    def __init__(self, in_dim: int, out_dim: int, n_realities: int = 4):
        super().__init__("QuantumInterference")
        self.n_realities = n_realities
        self.in_dim = in_dim
        self.out_dim = out_dim
        
        limit = np.sqrt(6 / (in_dim + out_dim))
        for i in range(n_realities):
            self.params[f'W{i}'] = np.random.uniform(-limit, limit, (in_dim, out_dim))
            self.params[f'b{i}'] = np.zeros(out_dim)
            self.params[f'phase{i}'] = np.random.uniform(0, 2*np.pi)
        
        self.params['mixing'] = np.zeros(n_realities)
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache['input'] = x
        
        amplitudes = []
        for i in range(self.n_realities):
            amp = x @ self.params[f'W{i}'] + self.params[f'b{i}']
            amp = amp * np.exp(1j * self.params[f'phase{i}'])
            amplitudes.append(amp)
        
        self.cache['amplitudes'] = amplitudes
        
        # Interference pattern (sum of complex amplitudes, take magnitude)
        weights = stable_softmax(self.params['mixing'])
        superposition = sum(w * a for w, a in zip(weights, amplitudes))
        
        # Output is real part + magnitude
        output = np.real(superposition) + 0.1 * np.abs(superposition)
        return output.astype(np.float64)
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        x = self.cache['input']
        weights = stable_softmax(self.params['mixing'])
        
        dx = np.zeros_like(x)
        self.grads['mixing'] = np.zeros(self.n_realities)
        
        for i in range(self.n_realities):
            self.grads[f'W{i}'] = x.T @ grad if x.ndim > 1 else np.outer(x, grad)
            self.grads[f'b{i}'] = np.sum(grad, axis=0) if grad.ndim > 1 else grad
            self.grads[f'phase{i}'] = 0.0
            dx += weights[i] * (grad @ self.params[f'W{i}'].T)
        
        return dx


# ═══════════════════════════════════════════════════════════════════════════════
# ENTANGLEMENT LAYER
# ═══════════════════════════════════════════════════════════════════════════════

class EntanglementLayer(Layer):
    """
    Pairwise feature entanglement layer.
    """
    
    def __init__(self, dim: int):
        super().__init__("Entanglement")
        self.dim = dim
        n_pairs = dim // 2
        
        self.params['coupling'] = np.random.uniform(-0.1, 0.1, n_pairs)
        self.params['phases'] = np.random.uniform(0, 2*np.pi, n_pairs)
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache['input'] = x
        result = x.copy()
        
        n_pairs = self.dim // 2
        for i in range(n_pairs):
            idx1, idx2 = 2*i, 2*i + 1
            if x.ndim == 1:
                a, b = x[idx1], x[idx2]
            else:
                a, b = x[:, idx1], x[:, idx2]
            
            coupling = self.params['coupling'][i]
            phase = self.params['phases'][i]
            
            # Entangled transformation
            entangled_a = a * np.cos(coupling) + b * np.sin(coupling) * np.exp(1j * phase)
            entangled_b = b * np.cos(coupling) - a * np.sin(coupling) * np.exp(-1j * phase)
            
            if x.ndim == 1:
                result[idx1] = np.real(entangled_a)
                result[idx2] = np.real(entangled_b)
            else:
                result[:, idx1] = np.real(entangled_a)
                result[:, idx2] = np.real(entangled_b)
        
        return result
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        x = self.cache['input']
        dx = grad.copy()
        
        n_pairs = self.dim // 2
        self.grads['coupling'] = np.zeros(n_pairs)
        self.grads['phases'] = np.zeros(n_pairs)
        
        for i in range(n_pairs):
            idx1, idx2 = 2*i, 2*i + 1
            coupling = self.params['coupling'][i]
            
            if x.ndim == 1:
                a, b = x[idx1], x[idx2]
                g1, g2 = grad[idx1], grad[idx2]
            else:
                a, b = x[:, idx1], x[:, idx2]
                g1, g2 = grad[:, idx1], grad[:, idx2]
            
            # Approximate gradients
            self.grads['coupling'][i] = np.sum(-a * np.sin(coupling) * g1 + b * np.cos(coupling) * g1)
            
            if x.ndim == 1:
                dx[idx1] = g1 * np.cos(coupling) - g2 * np.sin(coupling)
                dx[idx2] = g1 * np.sin(coupling) + g2 * np.cos(coupling)
            else:
                dx[:, idx1] = g1 * np.cos(coupling) - g2 * np.sin(coupling)
                dx[:, idx2] = g1 * np.sin(coupling) + g2 * np.cos(coupling)
        
        return dx


# ═══════════════════════════════════════════════════════════════════════════════
# RESIDUAL BLOCK
# ═══════════════════════════════════════════════════════════════════════════════

class ResidualBlock:
    """Residual block wrapping a sequence of layers"""
    
    def __init__(self, layers: List[Layer], scale: float = 0.5):
        self.name = "ResidualBlock"
        self.sublayers = layers
        self.scale = scale
        self.training = True
        self.cache = {}
        self._param_lr_scale = {}
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache['input'] = x
        out = x
        for layer in self.sublayers:
            out = layer.forward(out)
        return x + self.scale * out
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        dx = grad * self.scale
        for layer in reversed(self.sublayers):
            dx = layer.backward(dx)
        return grad + dx
    
    def set_training(self, mode: bool):
        self.training = mode
        for layer in self.sublayers:
            layer.set_training(mode)
    
    @property
    def params(self):
        all_params = {}
        for i, layer in enumerate(self.sublayers):
            for name, param in layer.params.items():
                all_params[f'{layer.name}_{i}_{name}'] = param
        return all_params
    
    @property
    def grads(self):
        all_grads = {}
        for i, layer in enumerate(self.sublayers):
            for name, grad in layer.grads.items():
                all_grads[f'{layer.name}_{i}_{name}'] = grad
        return all_grads
    
    def get_param_lr_scale(self, param_name):
        return self._param_lr_scale.get(param_name, 1.0)


# ═══════════════════════════════════════════════════════════════════════════════
# XENOMORPHIC v3.0 NETWORK - HOLORAID EDITION
# ═══════════════════════════════════════════════════════════════════════════════

class XenomorphicHoloRAID:
    """
    Xenomorphic Neural Network v3.0 with HoloRAID encoding.
    
    Architecture:
    1. Input projection with SafeGear bijective transformation
    2. HoloRAID encoding for distributed holographic representation
    3. Quantum interference with multiple realities
    4. CRT reconstruction for information fusion
    5. HyperMorphic fractal activation
    6. Final classification head
    """
    
    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int,
                 n_shards: int = 5, n_gears: int = 3, dropout: float = 0.1):
        """
        Initialize Xenomorphic HoloRAID network.
        
        Args:
            in_dim: Input feature dimension
            hidden_dim: Hidden layer dimension
            out_dim: Output (number of classes)
            n_shards: HoloRAID shards
            n_gears: SafeGear chain length
            dropout: Dropout rate
        """
        self.in_dim = in_dim
        self.hidden_dim = hidden_dim
        self.out_dim = out_dim
        
        # Build network
        self.layers = []
        
        # Stage 1: Input projection + SafeGear
        self.layers.append(Dense(in_dim, hidden_dim))
        self.layers.append(LayerNorm(hidden_dim))
        self.layers.append(Activation('gelu'))
        self.layers.append(SafeGearLayer(hidden_dim, n_gears=n_gears))
        self.layers.append(Dropout(dropout))
        
        # Stage 2: HoloRAID encoding block
        holoraid_block = [
            HoloRAIDLayer(hidden_dim, n_shards=n_shards, k_threshold=3),
            LayerNorm(hidden_dim),
            HyperMorphicLayer(hidden_dim, fractal_depth=3),
        ]
        self.layers.append(ResidualBlock(holoraid_block, scale=0.5))
        
        # Stage 3: Quantum interference block
        quantum_block = [
            QuantumInterferenceLayer(hidden_dim, hidden_dim, n_realities=4),
            LayerNorm(hidden_dim),
            EntanglementLayer(hidden_dim),
        ]
        self.layers.append(ResidualBlock(quantum_block, scale=0.5))
        self.layers.append(Dropout(dropout))
        
        # Stage 4: CRT reconstruction block
        crt_block = [
            CRTReconstructionLayer(hidden_dim, n_views=3),
            LayerNorm(hidden_dim),
            HyperMorphicLayer(hidden_dim, fractal_depth=2),
        ]
        self.layers.append(ResidualBlock(crt_block, scale=0.5))
        
        # Stage 5: Output head
        self.layers.append(Dense(hidden_dim, hidden_dim // 2))
        self.layers.append(LayerNorm(hidden_dim // 2))
        self.layers.append(Activation('gelu'))
        self.layers.append(Dropout(dropout))
        self.layers.append(Dense(hidden_dim // 2, out_dim))
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        for layer in self.layers:
            x = layer.forward(x)
        return x
    
    def backward(self, grad: np.ndarray) -> np.ndarray:
        for layer in reversed(self.layers):
            grad = layer.backward(grad)
        return grad
    
    def set_training(self, mode: bool):
        for layer in self.layers:
            layer.set_training(mode)
    
    def get_all_params(self) -> Dict[str, np.ndarray]:
        all_params = {}
        for i, layer in enumerate(self.layers):
            if hasattr(layer, 'sublayers'):  # ResidualBlock
                for j, sublayer in enumerate(layer.sublayers):
                    for name, param in sublayer.params.items():
                        all_params[f'{i}_{j}_{sublayer.name}_{name}'] = param
            else:
                for name, param in layer.params.items():
                    all_params[f'{i}_{layer.name}_{name}'] = param
        return all_params
    
    def get_all_grads(self) -> Dict[str, np.ndarray]:
        all_grads = {}
        for i, layer in enumerate(self.layers):
            if hasattr(layer, 'sublayers'):  # ResidualBlock
                for j, sublayer in enumerate(layer.sublayers):
                    for name, grad in sublayer.grads.items():
                        all_grads[f'{i}_{j}_{sublayer.name}_{name}'] = grad
            else:
                for name, grad in layer.grads.items():
                    all_grads[f'{i}_{layer.name}_{name}'] = grad
        return all_grads
    
    def count_params(self) -> int:
        total = 0
        for layer in self.layers:
            if hasattr(layer, 'sublayers'):
                for sublayer in layer.sublayers:
                    for param in sublayer.params.values():
                        if hasattr(param, 'size'):
                            total += param.size
                        else:
                            total += 1
            else:
                for param in layer.params.values():
                    if hasattr(param, 'size'):
                        total += param.size
                    else:
                        total += 1
        return total


# ═══════════════════════════════════════════════════════════════════════════════
# ADAMW OPTIMIZER WITH COSINE ANNEALING
# ═══════════════════════════════════════════════════════════════════════════════

class AdamW:
    def __init__(self, lr: float = 1e-3, beta1: float = 0.9, beta2: float = 0.999,
                 eps: float = 1e-8, weight_decay: float = 0.01):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.weight_decay = weight_decay
        self.m = {}
        self.v = {}
        self.t = 0
    
    def step(self, params: Dict[str, np.ndarray], grads: Dict[str, np.ndarray],
             lr_scale: float = 1.0):
        self.t += 1
        
        for name, param in params.items():
            if name not in grads or grads[name] is None:
                continue
            
            grad = grads[name]
            
            # Handle scalar parameters
            if not hasattr(param, 'shape'):
                continue
            if not hasattr(grad, 'shape'):
                grad = np.array(grad)
            if grad.shape != param.shape:
                continue
            
            if name not in self.m:
                self.m[name] = np.zeros_like(param)
                self.v[name] = np.zeros_like(param)
            
            # Update biased moments
            self.m[name] = self.beta1 * self.m[name] + (1 - self.beta1) * grad
            self.v[name] = self.beta2 * self.v[name] + (1 - self.beta2) * (grad ** 2)
            
            # Bias correction
            m_hat = self.m[name] / (1 - self.beta1 ** self.t)
            v_hat = self.v[name] / (1 - self.beta2 ** self.t)
            
            # Update with AdamW (decoupled weight decay)
            lr_eff = self.lr * lr_scale
            if 'W' in name or 'weight' in name.lower():
                param -= lr_eff * self.weight_decay * param
            
            param -= lr_eff * m_hat / (np.sqrt(v_hat) + self.eps)


def cosine_annealing(step: int, total_steps: int, lr_max: float, 
                     lr_min: float = 1e-6) -> float:
    """Cosine annealing learning rate schedule"""
    return lr_min + 0.5 * (lr_max - lr_min) * (1 + np.cos(np.pi * step / total_steps))


# ═══════════════════════════════════════════════════════════════════════════════
# DATA GENERATION
# ═══════════════════════════════════════════════════════════════════════════════

def generate_data(n_samples: int = 2000, n_features: int = 64, 
                  n_classes: int = 8, complexity: str = 'complex') -> Tuple[np.ndarray, np.ndarray]:
    """Generate synthetic classification data with varying complexity"""
    
    np.random.seed(42)
    
    if complexity == 'simple':
        # Gaussian clusters
        X = []
        y = []
        for c in range(n_classes):
            center = np.random.randn(n_features) * 3
            samples = center + np.random.randn(n_samples // n_classes, n_features) * 0.5
            X.append(samples)
            y.extend([c] * (n_samples // n_classes))
        X = np.vstack(X)
        y = np.array(y)
        
    elif complexity == 'medium':
        # Spiral patterns
        X = []
        y = []
        for c in range(n_classes):
            t = np.linspace(0, 4*np.pi, n_samples // n_classes) + c * 2 * np.pi / n_classes
            r = t / (4 * np.pi)
            base = np.column_stack([r * np.cos(t), r * np.sin(t)])
            noise = np.random.randn(n_samples // n_classes, 2) * 0.1
            features_2d = base + noise
            
            # Expand to n_features
            features = np.zeros((n_samples // n_classes, n_features))
            features[:, :2] = features_2d
            for i in range(2, n_features):
                features[:, i] = np.sin(features[:, i % 2] * (i + 1)) * np.cos(features[:, (i+1) % 2] * i)
            
            X.append(features)
            y.extend([c] * (n_samples // n_classes))
        X = np.vstack(X)
        y = np.array(y)
        
    elif complexity == 'complex':
        # XOR-like with non-linear transforms
        X = np.random.randn(n_samples, n_features)
        
        # Create complex decision boundaries
        projections = []
        for i in range(n_classes):
            proj = np.random.randn(n_features)
            proj /= np.linalg.norm(proj)
            projections.append(proj)
        
        scores = np.zeros((n_samples, n_classes))
        for i, proj in enumerate(projections):
            linear = X @ proj
            scores[:, i] = np.sin(linear * PHI) + np.cos(linear * PLASTIC) + linear * 0.1
            scores[:, i] += np.sum(X ** 2, axis=1) * 0.01 * ((-1) ** i)
        
        y = np.argmax(scores, axis=1)
        
    else:  # extreme
        # Fractal-like boundaries
        X = np.random.randn(n_samples, n_features)
        
        # Multi-scale fractal features
        fractal = np.zeros(n_samples)
        for d in range(5):
            scale = PHI ** d
            proj = np.random.randn(n_features)
            fractal += np.sin(X @ proj * scale) / scale
        
        # Additional non-linear mixing
        for i in range(3):
            p1 = np.random.randn(n_features)
            p2 = np.random.randn(n_features)
            fractal += np.sin(X @ p1) * np.cos(X @ p2) * 0.5
        
        # Discretize to classes
        fractal_norm = (fractal - fractal.min()) / (fractal.max() - fractal.min() + 1e-10)
        y = (fractal_norm * n_classes).astype(int)
        y = np.clip(y, 0, n_classes - 1)
    
    # Shuffle
    idx = np.random.permutation(len(y))
    return X[idx], y[idx]


# ═══════════════════════════════════════════════════════════════════════════════
# TRAINING AND EVALUATION
# ═══════════════════════════════════════════════════════════════════════════════

def cross_entropy_loss(logits: np.ndarray, targets: np.ndarray) -> Tuple[float, np.ndarray]:
    """Compute cross-entropy loss and gradient"""
    probs = stable_softmax(logits)
    n = len(targets)
    
    # Loss
    log_probs = np.log(probs + 1e-10)
    loss = -np.mean(log_probs[np.arange(n), targets])
    
    # Gradient
    grad = probs.copy()
    grad[np.arange(n), targets] -= 1
    grad /= n
    
    return loss, grad


def accuracy(logits: np.ndarray, targets: np.ndarray) -> float:
    """Compute classification accuracy"""
    preds = np.argmax(logits, axis=-1)
    return np.mean(preds == targets)


def feature_diversity(features: np.ndarray) -> float:
    """Measure diversity of feature representations"""
    if features.ndim == 1:
        return 1.0
    
    # Correlation matrix
    corr = np.corrcoef(features.T)
    
    # Off-diagonal correlations
    mask = ~np.eye(corr.shape[0], dtype=bool)
    off_diag = np.abs(corr[mask])
    
    # Diversity = 1 - mean correlation
    return 1.0 - np.nanmean(off_diag)


def train_network(network, X_train, y_train, X_test, y_test,
                  epochs: int = 100, batch_size: int = 64,
                  lr: float = 3e-4, weight_decay: float = 0.01,
                  patience: int = 15, verbose: bool = True) -> Dict[str, Any]:
    """Train network with AdamW and cosine annealing"""
    
    optimizer = AdamW(lr=lr, weight_decay=weight_decay)
    
    best_test_acc = 0.0
    best_epoch = 0
    no_improve = 0
    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}
    
    n_samples = len(X_train)
    n_batches = (n_samples + batch_size - 1) // batch_size
    total_steps = epochs * n_batches
    step = 0
    
    for epoch in range(epochs):
        network.set_training(True)
        
        # Shuffle
        idx = np.random.permutation(n_samples)
        X_shuffled = X_train[idx]
        y_shuffled = y_train[idx]
        
        epoch_loss = 0.0
        epoch_correct = 0
        
        for batch in range(n_batches):
            start = batch * batch_size
            end = min(start + batch_size, n_samples)
            
            X_batch = X_shuffled[start:end]
            y_batch = y_shuffled[start:end]
            
            # Forward
            logits = network.forward(X_batch)
            loss, grad = cross_entropy_loss(logits, y_batch)
            
            # Backward
            network.backward(grad)
            
            # Update with cosine annealing
            lr_scale = cosine_annealing(step, total_steps, 1.0, 0.01)
            optimizer.step(network.get_all_params(), network.get_all_grads(), lr_scale)
            
            epoch_loss += loss * len(y_batch)
            epoch_correct += np.sum(np.argmax(logits, axis=-1) == y_batch)
            step += 1
        
        train_loss = epoch_loss / n_samples
        train_acc = epoch_correct / n_samples
        
        # Evaluate
        network.set_training(False)
        test_logits = network.forward(X_test)
        test_acc = accuracy(test_logits, y_test)
        
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['test_acc'].append(test_acc)
        
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            best_epoch = epoch
            no_improve = 0
        else:
            no_improve += 1
        
        if verbose and (epoch + 1) % 10 == 0:
            print(f"  Epoch {epoch+1:3d}: loss={train_loss:.4f}, train_acc={train_acc:.4f}, test_acc={test_acc:.4f}")
        
        if no_improve >= patience:
            if verbose:
                print(f"  Early stopping at epoch {epoch+1}")
            break
    
    return {
        'best_test_acc': best_test_acc,
        'best_epoch': best_epoch,
        'final_train_acc': train_acc,
        'history': history
    }


# ═══════════════════════════════════════════════════════════════════════════════
# HOLORAID DEMONSTRATION
# ═══════════════════════════════════════════════════════════════════════════════

def demonstrate_holoraid():
    """Demonstrate HoloRAID encoding and reconstruction"""
    
    print("\n" + "="*80)
    print("  HOLORAID DEMONSTRATION - CRT-Based Erasure Coding")
    print("="*80)
    
    # Create test data (smaller values for demonstration)
    np.random.seed(42)
    original_data = np.random.rand(16).astype(np.float64)
    
    print(f"\n[1] Original Data (16 float values):")
    print(f"    {original_data[:8]}")
    print(f"    {original_data[8:]}")
    
    # Initialize HoloRAID with larger primes
    # Using primes starting at index 15: [53, 59, 61, 67, 71, 73, 79]
    # Product of smallest 3: 53 * 59 * 61 = 190,747 > 65,535
    holoraid = HoloRAID(n_shards=7, k_threshold=3, prime_offset=15, quantization_bits=16)
    
    print(f"\n[2] HoloRAID Configuration:")
    print(f"    Total shards: {holoraid.n_shards}")
    print(f"    Threshold (k): {holoraid.k_threshold}")
    print(f"    Prime moduli: {holoraid.primes}")
    print(f"    Min modulus (k primes): {holoraid.modulus_k:,}")
    print(f"    Max quantized value: {holoraid.max_quant:,}")
    print(f"    Failure tolerance: {holoraid.get_failure_tolerance()} shards can fail")
    print(f"    Redundancy factor: {holoraid.get_redundancy_factor():.2f}x")
    
    # Encode
    shards = holoraid.encode(original_data)
    
    print(f"\n[3] Encoded Shards (residues mod each prime):")
    for i, (shard, p) in enumerate(zip(shards, holoraid.primes)):
        print(f"    Shard {i} (mod {p:2d}): {shard[:4]}...")
    
    # Reconstruct from different k-subsets
    print(f"\n[4] Reconstruction Tests (using k={holoraid.k_threshold} shards):")
    
    test_subsets = [
        [0, 1, 2],      # First 3 shards
        [0, 3, 6],      # Spread out
        [4, 5, 6],      # Last 3 shards
        [1, 3, 5],      # Odd indices
    ]
    
    for subset in test_subsets:
        subset_shards = [shards[i] for i in subset]
        reconstructed = holoraid.decode(subset_shards, subset)
        
        error = np.max(np.abs(original_data - reconstructed))
        match = "✓ EXACT" if error < 1e-4 else f"✗ error={error:.6f}"
        
        primes_used = [int(holoraid.primes[i]) for i in subset]
        print(f"    Shards {subset} (primes {primes_used}): {match}")
    
    print("\n" + "-"*80)
    print("  HoloRAID successfully demonstrates holographic properties:")
    print("  • Any 3 of 7 shards can exactly reconstruct the original")
    print("  • Each shard is a 'holographic view' of the complete data")
    print("  • This is the CRT's natural holographic encoding!")
    print("-"*80)


def demonstrate_safegear():
    """Demonstrate SafeGear bijective transformations"""
    
    print("\n" + "="*80)
    print("  SAFEGEAR DEMONSTRATION - Bijective Winding Transformations")
    print("="*80)
    
    # Create SafeGear with coprime pair
    gear = SafeGear(7, 11)
    
    print(f"\n[1] SafeGear Configuration:")
    print(f"    Teeth A: {gear.teeth_a}")
    print(f"    Teeth B: {gear.teeth_b}")
    print(f"    Combined modulus: {gear.modulus}")
    print(f"    Gear ratio: {gear.gear_ratio:.4f}")
    
    # Test discrete winding/unwinding
    print(f"\n[2] Discrete Winding (bijective mod {gear.modulus}):")
    test_values = np.array([0, 1, 5, 10, 25, 50, 76])
    wound = gear.wind(test_values)
    unwound = gear.unwind(wound)
    
    for v, w, u in zip(test_values, wound, unwound):
        match = "✓" if v == u else "✗"
        print(f"    {v:2d} → wind → {int(w):2d} → unwind → {int(u):2d} {match}")
    
    # Test continuous winding
    print(f"\n[3] Continuous Winding (differentiable):")
    x = np.array([0.0, 0.5, 1.0, 2.0, -1.0])
    y = gear.continuous_wind(x)
    x_recovered = gear.continuous_unwind(y)
    
    for xi, yi, xr in zip(x, y, x_recovered):
        error = abs(xi - xr)
        match = "✓" if error < 1e-4 else f"✗ err={error:.4f}"
        print(f"    {xi:5.2f} → {yi:6.3f} → {xr:6.3f} {match}")
    
    # Test gear chain
    print(f"\n[4] SafeGear Chain (multiple gears):")
    chain = SafeGearChain([(3, 5), (5, 7), (7, 11)])
    print(f"    Chain: (3,5) → (5,7) → (7,11)")
    print(f"    Total modulus: {chain.total_modulus}")
    
    x_test = np.array([0.0, 1.0, -0.5, 2.5])
    y_chain = chain.wind(x_test)
    x_chain = chain.unwind(y_chain)
    
    for xi, yi, xr in zip(x_test, y_chain, x_chain):
        error = abs(xi - xr)
        match = "✓" if error < 1e-3 else f"✗"
        print(f"    {xi:5.2f} → {yi:7.4f} → {xr:7.4f} {match}")
    
    print("\n" + "-"*80)
    print("  SafeGear demonstrates bijective properties:")
    print("  • Coprime pairs guarantee reversibility (Extended Euclidean Algorithm)")
    print("  • Continuous version provides differentiable transformations")
    print("  • Chains compose multiple gears for complex mappings")
    print("-"*80)


# ═══════════════════════════════════════════════════════════════════════════════
# MAIN BENCHMARK
# ═══════════════════════════════════════════════════════════════════════════════

def run_benchmark():
    """Run full benchmark of Xenomorphic HoloRAID network"""
    
    print("\n" + "="*80)
    print("  XENOMORPHIC v3.0 - HOLORAID NEURAL NETWORK BENCHMARK")
    print("="*80)
    
    # Demonstrate primitives
    demonstrate_safegear()
    demonstrate_holoraid()
    
    # Generate data
    print("\n" + "="*80)
    print("  NEURAL NETWORK TRAINING BENCHMARK")
    print("="*80)
    
    complexities = ['simple', 'medium', 'complex', 'extreme']
    results = {}
    
    for complexity in complexities:
        print(f"\n{'─'*80}")
        print(f"  Task: {complexity.upper()}")
        print(f"{'─'*80}")
        
        # Generate data
        X, y = generate_data(n_samples=2000, n_features=64, n_classes=8, complexity=complexity)
        
        # Train/test split
        split = int(0.8 * len(X))
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        print(f"  Data: {len(X_train)} train, {len(X_test)} test, {X.shape[1]} features, 8 classes")
        
        # Create and train network
        print(f"\n  Training XenomorphicHoloRAID...")
        network = XenomorphicHoloRAID(
            in_dim=64, 
            hidden_dim=128, 
            out_dim=8,
            n_shards=5,
            n_gears=3,
            dropout=0.1
        )
        
        print(f"  Parameters: {network.count_params():,}")
        
        start_time = time.time()
        result = train_network(
            network, X_train, y_train, X_test, y_test,
            epochs=100, batch_size=64, lr=3e-4,
            patience=20, verbose=True
        )
        train_time = time.time() - start_time
        
        # Compute feature diversity
        network.set_training(False)
        hidden_features = network.layers[3].forward(
            network.layers[2].forward(
                network.layers[1].forward(
                    network.layers[0].forward(X_test)
                )
            )
        )
        diversity = feature_diversity(hidden_features)
        
        results[complexity] = {
            'test_acc': result['best_test_acc'],
            'train_acc': result['final_train_acc'],
            'diversity': diversity,
            'time': train_time,
            'params': network.count_params()
        }
        
        print(f"\n  Results:")
        print(f"    Best Test Accuracy: {result['best_test_acc']*100:.2f}%")
        print(f"    Final Train Accuracy: {result['final_train_acc']*100:.2f}%")
        print(f"    Feature Diversity: {diversity*100:.1f}%")
        print(f"    Training Time: {train_time:.1f}s")
    
    # Summary
    print("\n" + "="*80)
    print("  BENCHMARK SUMMARY")
    print("="*80)
    print(f"\n  {'Task':<10} {'Test Acc':>10} {'Train Acc':>10} {'Diversity':>10} {'Time':>8}")
    print("  " + "-"*50)
    for task, r in results.items():
        print(f"  {task:<10} {r['test_acc']*100:>9.2f}% {r['train_acc']*100:>9.2f}% {r['diversity']*100:>9.1f}% {r['time']:>7.1f}s")
    
    print("\n" + "="*80)
    print("  ✓ XENOMORPHIC v3.0 HOLORAID BENCHMARK COMPLETE")
    print("="*80)
    
    return results


if __name__ == "__main__":
    results = run_benchmark()


